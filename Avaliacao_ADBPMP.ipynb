{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flekT6GFDN6m"
      },
      "source": [
        "# <span style=\"color:blue\">MSc em Ciência de Dados</span>\n",
        "# <span style=\"color:blue\">Análise de Dados com Base em Processamento Massivo em Paralelo</span>\n",
        "\n",
        "## <span style=\"color:blue\">Avaliação Final - Notebook </span>\n",
        "\n",
        "**Material Produzido por:**<br>\n",
        ">**Profa. Dra. Cristina Dutra de Aguiar**<br>\n",
        "\n",
        "\n",
        "**CEMEAI - ICMC/USP São Carlos**\n",
        "\n",
        "Este *notebook* deve conter as respostas para as consultas analíticas solicitadas nas Questões 8, 9 e 10. É possível especificar as consultas analíticas usando Pandas, o método spark.sql() ou os métodos do módulo pyspark.sql. As consultas devem ser especificadas na seção 6, na célula indicada para resposta.\n",
        "\n",
        "**IMPORTANTE**\n",
        "\n",
        "- **As respostas para as Questões 8, 9 e 10 somente serão consideradas se forem especificadas no *notebook*. Portanto, independentemente da alternativa estar certa ou errada, se a consulta analítica correspondente não for especificada, a alternativa será considerada errada.**\n",
        "\n",
        "- **Caso uma alternativa esteja certa, porém a especificação da consulta analítica correspondente estiver errada no *notebook*, a alternativa será considerada errada.**\n",
        "\n",
        "\n",
        "O *notebook* contém a constelação de fatos da BI Solutions que deve ser utilizada para responder às questões e também todas as bibliotecas, bases de dados, inicializações, instalações, importações, geração de dataFrames, geração de visões temporárias e conversão dos tipos de dados necessárias para a realização da questão. Portanto, o *notebook* está preparado para ser executado usando Pandas, o método spark.sql() e os métodos do módulo pyspark.sql.\n",
        "\n",
        "O uso do *framework* Spark requer diversas configurações no ambiente de desenvolvimento para executar o *notebook*. Dado que tal complexidade foge do escopo de nossa disciplina, recomenda-se que o *notebook* seja executado na plataforma de desenvolvimento COLAB. O uso do COLAB  proporciona um ambiente de desenvolvimento pré-configurado e remove a complexidade de instalação e configuração de pacotes e *frameworks* que são utilizados na disciplina.\n",
        "\n",
        "**INSTRUÇÕES DE ENTREGA**\n",
        "\n",
        "**O que deve ser entregue:**\n",
        "- **O notebook com as respostas no formato .ipynb**\n",
        "- **O notebook com as respostas no formato .pdf**\n",
        "\n",
        "**Ambos arquivos devem ser nomeados usando o primeiro nome e o último sobrenome do aluno. Por exemplo: CristinaAguiar.ipynb e CristinaAguiar.pdf.**\n",
        "\n",
        "Boa avaliação!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o3dN_WLQcyD"
      },
      "source": [
        "#1 Constelação de Fatos da BI Solutions\n",
        "\n",
        "A aplicação de *data warehousing* da BI Solutions utiliza como base uma constelação de fatos, conforme descrita a seguir.\n",
        "\n",
        "**Tabelas de dimensão**\n",
        "\n",
        "- data (dataPK, dataCompleta, dataDia, dataMes, dataBimestre, dataTrimestre, dataSemestre, dataAno)\n",
        "- funcionario (funcPK, funcMatricula, funcNome, funcSexo, funcDataNascimento, funcDiaNascimento, funcMesNascimento, funcAnoNascimento, funcCidade, funcEstadoNome, funcEstadoSigla, funcRegiaoNome, funcRegiaoSigla, funcPaisNome, funcPaisSigla)\n",
        "- equipe (equipePK, equipeNome, filialNome, filialCidade, filialEstadoNome, filialEstadoSigla, filialRegiaoNome, filialRegiaoSigla, filialPaisNome, filialPaisSigla)\n",
        "- cargo (cargoPK, cargoNome, cargoRegimeTrabalho, cargoEscolaridadeMinima, cargoNivel)\n",
        "- cliente (clientePK, clienteNomeFantasia, clienteSetor, clienteCidade, clienteEstadoNome, clienteEstadoSigla, clienteRegiaoNome, clienteRegiaoSigla, clientePaisNome, clientePaisSigla)\n",
        "\n",
        "**Tabelas de fatos**\n",
        "- pagamento (dataPK, funcPK, equipePK, cargoPK, salario, quantidadeLancamentos)\n",
        "- negociacao (dataPK, equipePK, clientePK, receita, quantidadeNegociacoes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGeh8KdXwVCQ"
      },
      "source": [
        "#2 Obtenção dos Dados da BI Solutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCCNC64AzBG0"
      },
      "source": [
        "## 2.1 Baixando o Módulo wget\n",
        "\n",
        "Para baixar os dados referentes ao esquema relacional da constelação de fatos da BI Solutions, é utilizado o módulo  **wget**. O comando a seguir realiza a instalação desse módulo. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e0Eao1K0EYG"
      },
      "source": [
        "#instalando o módulo wget\n",
        "%%capture\n",
        "!pip install -q wget\n",
        "!mkdir data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j56pVJ2hZ2i5"
      },
      "source": [
        "## 2.2 Obtenção dos Dados das Tabelas de Dimensão\n",
        "\n",
        "Os comandos a seguir baixam os dados que povoam as tabelas de dimensão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46QzTpLJwfkW",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5bf87240-642b-479b-bace-8914e92e4eae"
      },
      "source": [
        "#baixando os dados das tabelas de dimensão\n",
        "import wget\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv\"\n",
        "wget.download(url, \"data/data.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv\"\n",
        "wget.download(url, \"data/funcionario.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv\"\n",
        "wget.download(url, \"data/equipe.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv\"\n",
        "wget.download(url, \"data/cargo.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv\"\n",
        "wget.download(url, \"data/cliente.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/cliente (1).csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o-dC7feszRc"
      },
      "source": [
        "## 2.3 Obtenção dos Dados Tabelas de Fatos\n",
        "\n",
        "Os comandos a seguir baixam os dados que povoam as tabelas de fatos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWM-CUFgBl_8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "13b2047b-6a41-4971-e9f2-3a45178e352a"
      },
      "source": [
        "#baixando os dados das tabelas de fatos\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv\"\n",
        "wget.download(url, \"data/pagamento.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv\"\n",
        "wget.download(url, \"data/negociacao.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/negociacao (1).csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO16-7-jOioq"
      },
      "source": [
        "# 3 Apache Spark Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVEgY9qKflBV"
      },
      "source": [
        "## 3.1 Instalação\n",
        "\n",
        "Neste *notebook* é criado um *cluster* Spark composto apenas por um **nó mestre**. Ou seja, o *cluster* não possui um ou mais **nós de trabalho** e o **gerenciador de cluster**. Nessa configuração, as tarefas (*tasks*) são realizadas no próprio *driver* localizado no **nó mestre**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaM-OnIjgLS2"
      },
      "source": [
        "Para que o cluster possa ser criado, primeiramente é instalado o Java Runtime Environment (JRE) versão 8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXls3bfoglKW"
      },
      "source": [
        "#instalando Java Runtime Environment (JRE) versão 8\n",
        "%%capture\n",
        "!apt-get remove openjdk*\n",
        "!apt-get update --fix-missing\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BQzZfDYhb4j"
      },
      "source": [
        "Na sequência, é feito o *download* do Apache Spark versão 3.0.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a_Yv59zg3gm"
      },
      "source": [
        "#baixando Apache Spark versão 3.0.0\n",
        "%%capture\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RETWX6wqhkLf"
      },
      "source": [
        "Na sequência, são configuradas as variáveis de ambiente JAVA_HOME e SPARK_HOME. Isto permite que tanto o Java quanto o Spark possam ser encontrados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZpR7NwOh2EB"
      },
      "source": [
        "import os\n",
        "#configurando a variável de ambiente JAVA_HOME\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#configurando a variável de ambiente SPARK_HOME\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql0z7Ro1iHQb"
      },
      "source": [
        "Por fim, são instalados dois pacotes da linguagem de programação Python, cujas funcionalidades são descritas a seguir.\n",
        "\n",
        "> **Pacote findspark:** Usado para ler a variável de ambiente SPARK_HOME e armazenar seu valor na variável dinâmica de ambiente PYTHONPATH. Como resultado, Python pode encontrar a instalação do Spark.\n",
        "\n",
        "> **Pacote pyspark:** PySpark é a API do Python para Spark. Ela possibilita o uso de Python, considerando que o *framework* Apache Spark encontra-se desenvolvido na linguagem de programação Scala."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oSYOwKljPf5"
      },
      "source": [
        "%%capture\n",
        "#instalando o pacote findspark\n",
        "!pip install -q findspark==1.4.2\n",
        "#instalando o pacote pyspark\n",
        "!pip install -q pyspark==3.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAaLyjPzmIwZ"
      },
      "source": [
        "## 3.2 Conexão\n",
        "\n",
        "PySpark não é adicionado ao *sys.path* por padrão. Isso significa que não é possível importá-lo, pois o interpretador da linguagem Python não sabe onde encontrá-lo.\n",
        "\n",
        "Para resolver esse aspecto, é necessário instalar o módulo `findspark`. Esse módulo mostra onde PySpark está localizado. Os comandos a seguir têm essa finalidade.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zm1pBTEmjp4"
      },
      "source": [
        "#importando o módulo findspark\n",
        "import findspark\n",
        "#carregando a variávels SPARK_HOME na variável dinâmica PYTHONPATH\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDqfefF7YUab"
      },
      "source": [
        "Depois de configurados os pacotes e módulos e inicializadas as variáveis de ambiente, é possível iniciar o uso do Spark na aplicação de `data warehousing`. Para tanto, é necessário importar o comando `SparkSession` do módulo `pyspark.sql`. São utilizados os seguintes conceitos: <br>\n",
        "\n",
        "- `SparkSession`: permite a criação de `DataFrames`. Como resultado, as tabelas relacionais podem ser manipuladas por meio de `DataFrames` e é possível realizar consultas OLAP por meio de comandos SQL. <br>\n",
        "- `builder`: cria uma instância de SparkSession. <br>\n",
        "- `appName`: define um nome para a aplicação, o qual pode ser visto na interface de usuário web do Spark. <br>\n",
        "- `master`: define onde está o nó mestre do *cluster*. Como a aplicação é executada localmente e não em um *cluster*, indica-se isso pela *string* `local` seguida do parâmetro `[*]`. Ou seja, define-se que apenas núcleos locais são utilizados.\n",
        "- `getOrCreate`: cria uma SparkSession. Caso ela já exista, retorna a instância existente.\n",
        "\n",
        "\n",
        "**Observação**: A lista completa de todos os parâmetros que podem ser utilizados na inicialização do *cluster* pode ser encontrada neste [link](https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TxljJ_cwBCy"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IpYfoG_kx_8"
      },
      "source": [
        "# 4 Geração dos DataFrames em Pandas da BI Solutions\n",
        "\n",
        "Nesta seção são gerados os DataFrames em Pandas. Atenção aos nomes desses DataFrames.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9arYf_PHlJCR"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw4NfyDZ--6z"
      },
      "source": [
        "cargoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv')\n",
        "clientePandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv')\n",
        "dataPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv')\n",
        "equipePandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv')\n",
        "funcionarioPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv')\n",
        "negociacaoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv')\n",
        "pagamentoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qL9SiR_pQE2"
      },
      "source": [
        "# 5 Geração dos DataFrames em Spark da BI Solutions\n",
        "\n",
        "Nesta seção são gerados dos DataFrames em Spark. Atenção aos nomes desses DataFrames.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRVoz-SGt87W"
      },
      "source": [
        "## 5.1 Criação dos DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "FNR-3dV6oYk4"
      },
      "source": [
        "#criando os DataFrames em Spark\n",
        "cargo = spark.read.csv(path=\"data/cargo.csv\", header=True, sep=\",\")\n",
        "cliente = spark.read.csv(path=\"data/cliente.csv\", header=True, sep=\",\")\n",
        "data = spark.read.csv(path=\"data/data.csv\", header=True, sep=\",\")\n",
        "equipe = spark.read.csv(path=\"data/equipe.csv\", header=True, sep=\",\")\n",
        "funcionario = spark.read.csv(path=\"data/funcionario.csv\", header=True, sep=\",\")\n",
        "negociacao = spark.read.csv(path=\"data/negociacao.csv\", header=True, sep=\",\")\n",
        "pagamento = spark.read.csv(path=\"data/pagamento.csv\", header=True, sep=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrch9vLgjl_H"
      },
      "source": [
        "## 5.2 Atualização dos Tipos de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A_ot2pOjsWB"
      },
      "source": [
        "Nos comandos a seguir, primeiro são identificados quais colunas de quais `DataFrames` devem ser do tipo de dado inteiro. Na sequência, ocorre a conversão. Por fim, são exibidos os esquemas dos `DataFrames`, possibilitando visualizar a mudança de tipo de dados das colunas especificadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmCV6Mur__z6"
      },
      "source": [
        "# identificando quais colunas de quais DataFrames devem ser do tipo de dado inteiro\n",
        "colunas_cargo = [\"cargoPK\"]\n",
        "colunas_cliente = [\"clientePK\"]\n",
        "colunas_data = [\"dataPK\", \"dataDia\", \"dataMes\", \"dataBimestre\", \"dataTrimestre\", \"dataSemestre\", \"dataAno\"]\n",
        "colunas_equipe = [\"equipePK\"]\n",
        "colunas_funcionario = [\"funcPK\", \"funcDiaNascimento\", \"funcMesNascimento\", \"funcAnoNascimento\"]\n",
        "colunas_negociacao = [\"equipePK\", \"clientePK\", \"dataPK\", \"quantidadeNegociacoes\"]\n",
        "colunas_pagamento = [\"funcPK\", \"equipePK\", \"dataPK\", \"cargoPK\", \"quantidadeLancamentos\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPNnDJcG9R5H"
      },
      "source": [
        "# importando o tipo de dado desejado\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "\n",
        "# atualizando o tipo de dado das colunas especificadas\n",
        "# substituindo as colunas já existentes\n",
        "\n",
        "for coluna in colunas_cargo:\n",
        "  cargo = cargo.withColumn(coluna, cargo[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_cliente:\n",
        "  cliente = cliente.withColumn(coluna, cliente[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_data:\n",
        "  data = data.withColumn(coluna, data[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_equipe:\n",
        "  equipe = equipe.withColumn(coluna, equipe[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_funcionario:\n",
        "  funcionario = funcionario.withColumn(coluna, funcionario[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_negociacao:\n",
        "  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_pagamento:\n",
        "  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(IntegerType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0dX_7U_AzIY"
      },
      "source": [
        "Nos comandos a seguir, primeiro são identificados quais colunas de quais `DataFrames` devem ser do tipo de dado número de ponto flutuante. Na sequência, ocorre a conversão. Por fim, são exibidos os esquemas dos `DataFrames`, possibilitando visualizar a mudança de tipo de dados das colunas especificadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBcQ7Ep7AWqN"
      },
      "source": [
        "# identificando quais colunas de quais DataFrames devem ser do tipo de dado número de ponto flutuante\n",
        "colunas_negociacao = [\"receita\"]\n",
        "colunas_pagamento = [\"salario\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcfvkIK1BRSp"
      },
      "source": [
        "# importando o tipo de dado desejado\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "\n",
        "# atualizando o tipo de dado das colunas especificadas\n",
        "# substituindo as colunas já existentes\n",
        "\n",
        "for coluna in colunas_negociacao:\n",
        "  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(FloatType()))\n",
        "\n",
        "for coluna in colunas_pagamento:\n",
        "  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(FloatType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91nwfsV3_rKs"
      },
      "source": [
        "# importando funções adicionais\n",
        "from pyspark.sql.functions import round, desc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wN5iOGKwnHG"
      },
      "source": [
        "## 5.3 Criação de Visões Temporárias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJsqRI3TwsjS",
        "outputId": "e09d5b4d-f0b2-4d15-8ad1-74b3e90f8300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "#criando as visões temporárias\n",
        "cargo.createOrReplaceTempView(\"cargo\")\n",
        "cliente.createOrReplaceTempView(\"cliente\")\n",
        "data.createOrReplaceTempView(\"data\")\n",
        "equipe.createOrReplaceTempView(\"equipe\")\n",
        "funcionario.createOrReplaceTempView(\"funcionario\")\n",
        "negociacao.createOrReplaceTempView(\"negociacao\")\n",
        "pagamento.createOrReplaceTempView(\"pagamento\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-670c0dadb4d7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#criando as visões temporárias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcargo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cargo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcliente\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cliente\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mequipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"equipe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cargo' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 Respostas"
      ],
      "metadata": {
        "id": "2X_PNhxFCKh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lembre-se que é possível especificar as consultas analíticas usando Pandas, o método spark.sql() ou os métodos do módulo pyspark.sql."
      ],
      "metadata": {
        "id": "vqsvtdy4CdVw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcSouuCk6N0-"
      },
      "source": [
        "## Especificação da Consulta Analítica da Questão 8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considere a constelação de fatos da BI Solutions e a seguinte solicitação de consulta:\n",
        "\n",
        "“Qual a quantidade de negociações por ano e pela cidade da equipe, considerando equipes localizadas na mesma cidade de seus clientes?” Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: ANO, CIDADE e TOTALNEGOCIACOES. Ordene as linhas exibidas primeiro por ano em ordem ascendente e depois por cidade em ordem ascendente.\n",
        "\n",
        "Considerando a resposta para a consulta analítica, assinale a alternativa correta:\n",
        "\n",
        "\n",
        "\n",
        "a.\n",
        "As equipes localizadas na cidade de SAO PAULO possuem mais negociações do que as demais equipes em todos os anos.\n",
        "\n",
        "\n",
        "b.\n",
        "Quatro cidades distintas possuem equipes que negociaram mais de uma vez em 2018.\n",
        "\n",
        "\n",
        "c.\n",
        "Três cidades distintas possuem equipes que negociaram mais de uma vez em 2019.\n",
        "\n",
        "\n",
        "d.\n",
        "As equipes localizadas na cidade do RIO DE JANEIRO possuem menos negociações do que as demais equipes em todos os anos.\n",
        "\n",
        "\n",
        "e.\n",
        "Com exceção do ano de 2020, as negociações realizadas pelas equipes localizadas na cidade de SAO PAULO aumentaram paulatinamente."
      ],
      "metadata": {
        "id": "5LyLZCTkmYe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Celula somente para testes para entendermos as bases\n",
        "\n",
        "negociacao.show(4)\n",
        "data.show(4)\n",
        "equipe.show(4)\n",
        "cliente.show(4)\n",
        "\n",
        "query = \"\"\"\n",
        "select distinct quantidadeNegociacoes\n",
        "from negociacao\n",
        "\"\"\"\n",
        "\n",
        "# Executa a query (consulta)\n",
        "result_df = spark.sql(query)\n",
        "\n",
        "# Mostra os resultados da consulta\n",
        "print(\"Resultado utilizando o método spark.sql():\")\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2_l9UEVnj1u",
        "outputId": "ea0078b0-c5bf-495b-abdc-2f7939c13178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+------+--------+---------------------+\n",
            "|equipePK|clientePK|dataPK| receita|quantidadeNegociacoes|\n",
            "+--------+---------+------+--------+---------------------+\n",
            "|       2|        9|    22|11564.75|                    1|\n",
            "|       2|       24|    11| 17990.5|                    1|\n",
            "|       2|       28|    21| 16335.9|                    1|\n",
            "|       1|       30|    23| 8495.55|                    1|\n",
            "+--------+---------+------+--------+---------------------+\n",
            "only showing top 4 rows\n",
            "\n",
            "+------+------------+-------+-------+------------+-------------+------------+-------+\n",
            "|dataPK|dataCompleta|dataDia|dataMes|dataBimestre|dataTrimestre|dataSemestre|dataAno|\n",
            "+------+------------+-------+-------+------------+-------------+------------+-------+\n",
            "|     1|    1/1/2016|      1|      1|           1|            1|           1|   2016|\n",
            "|     2|    2/1/2016|      2|      1|           1|            1|           1|   2016|\n",
            "|     3|    3/1/2016|      3|      1|           1|            1|           1|   2016|\n",
            "|     4|    4/1/2016|      4|      1|           1|            1|           1|   2016|\n",
            "+------+------------+-------+-------+------------+-------------+------------+-------+\n",
            "only showing top 4 rows\n",
            "\n",
            "+--------+-------------+--------------------+--------------+----------------+-----------------+----------------+-----------------+--------------+---------------+\n",
            "|equipePK|   equipeNome|          filialNome|  filialCidade|filialEstadoNome|filialEstadoSigla|filialRegiaoNome|filialRegiaoSigla|filialPaisNome|filialPaisSigla|\n",
            "+--------+-------------+--------------------+--------------+----------------+-----------------+----------------+-----------------+--------------+---------------+\n",
            "|       1|APP - DESKTOP|SAO PAULO - AV. P...|     SAO PAULO|       SAO PAULO|               SP|         SUDESTE|               SE|        BRASIL|             BR|\n",
            "|       2|APP - DESKTOP|RIO DE JANEIRO - ...|RIO DE JANEIRO|  RIO DE JANEIRO|               RJ|         SUDESTE|               SE|        BRASIL|             BR|\n",
            "|       3|          WEB|SAO PAULO - AV. P...|     SAO PAULO|       SAO PAULO|               SP|         SUDESTE|               SE|        BRASIL|             BR|\n",
            "|       4|          WEB|RIO DE JANEIRO - ...|RIO DE JANEIRO|  RIO DE JANEIRO|               RJ|         SUDESTE|               SE|        BRASIL|             BR|\n",
            "+--------+-------------+--------------------+--------------+----------------+-----------------+----------------+-----------------+--------------+---------------+\n",
            "only showing top 4 rows\n",
            "\n",
            "+---------+-------------------+-------------------+-------------+-----------------+------------------+-----------------+------------------+---------------+----------------+\n",
            "|clientePK|clienteNomeFantasia|       clienteSetor|clienteCidade|clienteEstadoNome|clienteEstadoSigla|clienteRegiaoNome|clienteRegiaoSigla|clientePaisNome|clientePaisSigla|\n",
            "+---------+-------------------+-------------------+-------------+-----------------+------------------+-----------------+------------------+---------------+----------------+\n",
            "|        1|           VIA FOOD|BEBIDAS E ALIMENTOS|    SAO PAULO|        SAO PAULO|                SP|          SUDESTE|                SE|         BRASIL|              BR|\n",
            "|        2|          VIA PIZZA|BEBIDAS E ALIMENTOS|    SAO PAULO|        SAO PAULO|                SP|          SUDESTE|                SE|         BRASIL|              BR|\n",
            "|        3|           VIA JAPA|BEBIDAS E ALIMENTOS|    SAO PAULO|        SAO PAULO|                SP|          SUDESTE|                SE|         BRASIL|              BR|\n",
            "|        4|            VIA VEG|BEBIDAS E ALIMENTOS|    SAO PAULO|        SAO PAULO|                SP|          SUDESTE|                SE|         BRASIL|              BR|\n",
            "+---------+-------------------+-------------------+-------------+-----------------+------------------+-----------------+------------------+---------------+----------------+\n",
            "only showing top 4 rows\n",
            "\n",
            "Resultado utilizando o método spark.sql():\n",
            "+---------------------+\n",
            "|quantidadeNegociacoes|\n",
            "+---------------------+\n",
            "|                    1|\n",
            "+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resposta da Questão 8\n",
        "# Não se esqueça de comentar detalhadamente a sua solução\n",
        "\n",
        "query = \"\"\"\n",
        "select dataAno as ANO,\n",
        "filialCidade as CIDADE,\n",
        "count(quantidadeNegociacoes) as TOTALNEGOCIACOES\n",
        "FROM negociacao as a\n",
        "INNER JOIN data as b\n",
        "ON a.dataPK = b.dataPK\n",
        "INNER JOIN equipe as c\n",
        "ON a.equipePK = c.equipePK\n",
        "INNER JOIN cliente as d\n",
        "ON a.clientePK = d.clientePK\n",
        "where d.clienteCidade = c.filialCidade\n",
        "group by ANO, CIDADE\n",
        "order by ANO asc, CIDADE asc, TOTALNEGOCIACOES\n",
        "\"\"\"\n",
        "\n",
        "# Executa a query (consulta)\n",
        "result_df = spark.sql(query)\n",
        "\n",
        "# Mostra os resultados da consulta\n",
        "print(\"Resultado utilizando o método spark.sql():\")\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "id": "4OjKqHvginUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a9d6c37-7e3e-43df-c39e-15f05a47883d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado utilizando o método spark.sql():\n",
            "+----+--------------+----------------+\n",
            "| ANO|        CIDADE|TOTALNEGOCIACOES|\n",
            "+----+--------------+----------------+\n",
            "|2016|RIO DE JANEIRO|             130|\n",
            "|2016|     SAO PAULO|             141|\n",
            "|2017|  CAMPO GRANDE|              99|\n",
            "|2017|RIO DE JANEIRO|             229|\n",
            "|2017|     SAO PAULO|             209|\n",
            "|2018|  CAMPO GRANDE|             198|\n",
            "|2018|RIO DE JANEIRO|             339|\n",
            "|2018|     SAO PAULO|             312|\n",
            "|2019|  CAMPO GRANDE|             183|\n",
            "|2019|        RECIFE|             175|\n",
            "|2019|RIO DE JANEIRO|             353|\n",
            "|2019|     SAO PAULO|             511|\n",
            "|2020|  CAMPO GRANDE|             209|\n",
            "|2020|        RECIFE|             146|\n",
            "|2020|RIO DE JANEIRO|             324|\n",
            "|2020|     SAO PAULO|             453|\n",
            "+----+--------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# .join(data, on=\"dataPK\")n\n",
        "# .join(equipe, on=\"equipePK\")n\n",
        "# .join(equipe, on=\"equipePK\")n\n",
        "# .where(\"dataPK BETWEEN 367 AND 731\")n\n",
        "# .select(\"equipeNome\", \"filialNome\", \"receita\")n\n",
        "# .groupBy(\"equipeNome\", \"filialNome\")n\n",
        "# .sum(\"receita\")n\n",
        "# .orderBy(desc(\"sum(receita)\"))"
      ],
      "metadata": {
        "id": "IrVcuG0gtSxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '### Utilizando métodos do módulo pyspark.sql.\n",
        "# Junção dos DataFrames\n",
        "# joined_df = (\n",
        "#     negociacao\n",
        "#     .join(equipe, \"equipePK\")\n",
        "#     .join(cliente, \"clientePK\")\n",
        "#     .join(data, \"dataPK\")\n",
        "#     .where(col(\"filialCidade\") == col(\"clienteCidade\"))\n",
        "#     .select(col(\"dataAno\").alias(\"ANO\"), col(\"filialCidade\").alias(\"CIDADE\")) # Usar somente Letras maiúsculas em nome de colunas não é uma boa prática\n",
        "# )\n",
        "\n",
        "\n",
        "# Agrupando e fazendo a contagem do 'TOTALDENEGOCIACOES'\n",
        "# result_df = (\n",
        "#     joined_df\n",
        "#     .groupBy(\"ANO\", \"CIDADE\")\n",
        "#     .count()\n",
        "#     .withColumnRenamed(\"count\", \"TOTALNEGOCIACOES\")\n",
        "#     .orderBy(\"ANO\", \"CIDADE\")\n",
        "# )\n",
        "\n",
        "# Mostra o resulta da consulta\n",
        "# print(\"\\nResultado utilizando métodos do módulo pyspark.sql:\")\n",
        "# result_df.show()'"
      ],
      "metadata": {
        "id": "finy1br-t-a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpFnRTk369Q5"
      },
      "source": [
        "## Especificação da Consulta Analítica da Questão 9"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considere a constelação de fatos da BI Solutions e a seguinte solicitação de consulta:\n",
        "\n",
        "“Liste todas as agregações que podem ser geradas a partir da média dos salários dos funcionários que moram na região SUDESTE por sexo e por ano.” Arredonde a média dos salários para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: SEXO, ANO e MEDIASALARIO. Ordene as linhas exibidas primeiro por sexo, depois por ano, depois por média dos salários, todos em ordem ascendente.\n",
        "\n",
        "Considerando a resposta para a consulta analítica, assinale a alternativa correta:\n",
        "\n",
        "\n",
        "a.\n",
        "São retornadas 10 linhas, das quais 5 linhas são referentes ao sexo feminino e 5 linhas são referentes ao sexo masculino.\n",
        "\n",
        "\n",
        "b.\n",
        "São retornadas 18 linhas, das quais 6 linhas são referentes ao sexo feminino e 6 linhas são referentes ao sexo masculino.\n",
        "\n",
        "\n",
        "c.\n",
        "A quarta e a quinta linhas retornadas contêm o mesmo valor de média dos salários e referem-se a anos diferentes.\n",
        "\n",
        "\n",
        "d.\n",
        "As médias dos salários das funcionárias de sexo feminino para os anos de 2017 e 2018 são menores do que as médias dos salários dos funcionários do sexo masculino para os anos de 2017 e 2018, respectivamente.\n",
        "\n",
        "\n",
        "e.\n",
        "São retornadas 13 linhas, das quais 6 linhas são referentes ao sexo feminino e 6 linhas são referentes ao sexo masculino."
      ],
      "metadata": {
        "id": "sedmFsYR1R1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pagamento.show(4)\n",
        "data.show(4)\n",
        "funcionario.show(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWy574aC4Cne",
        "outputId": "56928c51-2674-4eaf-d490-b4fc6d1758ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+------+-------+-------+---------------------+\n",
            "|funcPK|equipePK|dataPK|cargoPK|salario|quantidadeLancamentos|\n",
            "+------+--------+------+-------+-------+---------------------+\n",
            "|   147|       2|     5|     64|1559.94|                    1|\n",
            "|   124|       2|     5|    329|8102.77|                    1|\n",
            "|   175|       1|     5|    328|2532.51|                    1|\n",
            "|   171|       1|     5|    245| 7882.7|                    1|\n",
            "+------+--------+------+-------+-------+---------------------+\n",
            "only showing top 4 rows\n",
            "\n",
            "+------+------------+-------+-------+------------+-------------+------------+-------+\n",
            "|dataPK|dataCompleta|dataDia|dataMes|dataBimestre|dataTrimestre|dataSemestre|dataAno|\n",
            "+------+------------+-------+-------+------------+-------------+------------+-------+\n",
            "|     1|    1/1/2016|      1|      1|           1|            1|           1|   2016|\n",
            "|     2|    2/1/2016|      2|      1|           1|            1|           1|   2016|\n",
            "|     3|    3/1/2016|      3|      1|           1|            1|           1|   2016|\n",
            "|     4|    4/1/2016|      4|      1|           1|            1|           1|   2016|\n",
            "+------+------------+-------+-------+------------+-------------+------------+-------+\n",
            "only showing top 4 rows\n",
            "\n",
            "+------+-------------+-------------+--------+------------------+-----------------+-----------------+-----------------+-----------+--------------+---------------+--------------+---------------+------------+-------------+\n",
            "|funcPK|funcMatricula|     funcNome|funcSexo|funcDataNascimento|funcDiaNascimento|funcMesNascimento|funcAnoNascimento| funcCidade|funcEstadoNome|funcEstadoSigla|funcRegiaoNome|funcRegiaoSigla|funcPaisNome|funcPaisSigla|\n",
            "+------+-------------+-------------+--------+------------------+-----------------+-----------------+-----------------+-----------+--------------+---------------+--------------+---------------+------------+-------------+\n",
            "|     1|          M-1|ALINE ALMEIDA|       F|          1/1/1990|                1|                1|             1990|  SAO PAULO|     SAO PAULO|             SP|       SUDESTE|             SE|      BRASIL|           BR|\n",
            "|     2|          M-2|   ARAO ALVES|       M|          2/2/1990|                2|                2|             1990|   CAMPINAS|     SAO PAULO|             SP|       SUDESTE|             SE|      BRASIL|           BR|\n",
            "|     3|          M-3| ARON ANDRADE|       M|          3/3/1990|                3|                3|             1990|     SANTOS|     SAO PAULO|             SP|       SUDESTE|             SE|      BRASIL|           BR|\n",
            "|     4|          M-4|  ADA BARBOSA|       F|          4/4/1990|                4|                4|             1990|SANTO ANDRE|     SAO PAULO|             SP|       SUDESTE|             SE|      BRASIL|           BR|\n",
            "+------+-------------+-------------+--------+------------------+-----------------+-----------------+-----------------+-----------+--------------+---------------+--------------+---------------+------------+-------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resposta da Questão 9\n",
        "# Não se esqueça de comentar detalhadamente a sua solução\n",
        "\n",
        "query= '''SELECT\n",
        "        func.funcSexo as SEXO,\n",
        "        data.dataAno as ANO,\n",
        "        round(avg(pag.salario),2) as MEDIASALARIO\n",
        "        from pagamento pag\n",
        "        INNER JOIN data ON pag.dataPK = data.dataPK\n",
        "        inner join funcionario func on func.funcPK = pag.funcPK\n",
        "        where func.funcRegiaoNome ='SUDESTE'\n",
        "        group by CUBE (SEXO, ANO)\n",
        "        order by SEXO asc, ANO asc, MEDIASALARIO asc\n",
        "            '''\n",
        "\n",
        "result_df = spark.sql(query)\n",
        "\n",
        "result_df.show()"
      ],
      "metadata": {
        "id": "m98NQbOiio-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9226158-5a65-46d0-9834-a73925811aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+------------+\n",
            "|SEXO| ANO|MEDIASALARIO|\n",
            "+----+----+------------+\n",
            "|null|null|     7446.14|\n",
            "|null|2016|     6542.64|\n",
            "|null|2017|     7181.86|\n",
            "|null|2018|     7582.23|\n",
            "|null|2019|     7596.89|\n",
            "|null|2020|     7596.89|\n",
            "|   F|null|     7821.46|\n",
            "|   F|2016|      8587.4|\n",
            "|   F|2017|     8575.92|\n",
            "|   F|2018|     8333.57|\n",
            "|   F|2019|     7329.74|\n",
            "|   F|2020|     7329.74|\n",
            "|   M|null|     7322.23|\n",
            "|   M|2016|     5903.65|\n",
            "|   M|2017|     6763.65|\n",
            "|   M|2018|      7345.4|\n",
            "|   M|2019|     7689.92|\n",
            "|   M|2020|     7689.92|\n",
            "+----+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pagamento.join(funcionario, on=\"funcPK\").join(data, on=\"dataPK\")\\\n",
        "  .where(\"funcRegiaoNome = 'SUDESTE'\")\\\n",
        "  .select(\"funcSexo\", \"dataAno\", \"salario\")\\\n",
        "  .cube(\"funcSexo\", \"dataAno\").avg(\"salario\")\\\n",
        "  .withColumn(\"avg(salario)\", round(\"avg(salario)\", 2))\\\n",
        "  .orderBy(\"funcSexo\", \"dataAno\")\\\n",
        "  .withColumnRenamed(\"funcSexo\", \"SEXO\")\\\n",
        "  .withColumnRenamed(\"dataAno\", \"ANO\")\\\n",
        "  .withColumnRenamed(\"avg(salario)\", \"MEDIASALARIO\")\\\n",
        "  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXiHHFCj6Lg-",
        "outputId": "79c4e8b8-8933-45d0-dc31-15bc00f3ec9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+------------+\n",
            "|SEXO| ANO|MEDIASALARIO|\n",
            "+----+----+------------+\n",
            "|null|null|     7446.14|\n",
            "|null|2016|     6542.64|\n",
            "|null|2017|     7181.86|\n",
            "|null|2018|     7582.23|\n",
            "|null|2019|     7596.89|\n",
            "|null|2020|     7596.89|\n",
            "|   F|null|     7821.46|\n",
            "|   F|2016|      8587.4|\n",
            "|   F|2017|     8575.92|\n",
            "|   F|2018|     8333.57|\n",
            "|   F|2019|     7329.74|\n",
            "|   F|2020|     7329.74|\n",
            "|   M|null|     7322.23|\n",
            "|   M|2016|     5903.65|\n",
            "|   M|2017|     6763.65|\n",
            "|   M|2018|      7345.4|\n",
            "|   M|2019|     7689.92|\n",
            "|   M|2020|     7689.92|\n",
            "+----+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCpy46lL8Xdn"
      },
      "source": [
        "## Especificação da Consulta Analítica da Questão 10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considere a constelação de fatos da BI Solutions e a seguinte solicitação de consulta:\n",
        "\n",
        "\"Qual o lucro ou prejuízo médio de cada equipe localizada na região SUDESTE do BRASIL?\", sendo que lucro representa a diferença entre a média das receitas e a média dos salários. Arredonde o lucro ou prejuízo para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: NOME DA EQUIPE, NOME DA FILIAL, CIDADE DA FILIAL, LUCRO OU PREJUÍZO.\n",
        "\n",
        "Considerando a resposta para a consulta analítica, assinale a alternativa correta:\n",
        "\n",
        "\n",
        "a.\n",
        "As equipes localizadas na cidade de RIO DE JANEIRO garantem mais lucro à BI Solutions do que as equipes localizadas na cidade de SAO PAULO.\n",
        "\n",
        "\n",
        "b.\n",
        "Equipes que possuem WEB em seu nome, independentemente da filial e da cidade na qual estão localizadas, proveem mais lucro à BI Solutions do que as demais equipes.\n",
        "\n",
        "\n",
        "c.\n",
        "Os lucros gerados pelas equipes que possuem BI & ANALYTICS em seu nome, independentemente da filial e da cidade na qual estão localizadas, são maiores do que a soma dos lucros gerados pelas demais equipes.\n",
        "\n",
        "\n",
        "d.\n",
        "A soma dos lucros gerados pelas equipes que possuem APP - MOBILE em seu nome são maiores do que a soma dos lucros gerados pelas equipes que possuem APP - DESKTOP em seu nome, independentemente da filial e da cidade na qual estão localizadas.\n",
        "\n",
        "\n",
        "e.\n",
        "Todas as equipes tiveram lucro, independentemente da cidade na qual elas estão localizadas."
      ],
      "metadata": {
        "id": "ha_mbkoC9DYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resposta da Questão 10\n",
        "# Não se esqueça de comentar detalhadamente a sua solução\n",
        "\n",
        "# pagamento.printSchema()\n",
        "pagamento.show(4)\n",
        "negociacao.show(4)\n",
        "equipe.show(4)"
      ],
      "metadata": {
        "id": "dK3Ln6g_iqR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6ed61d-fa60-4361-e3fc-c3e062c9dc4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+------+-------+-------+---------------------+\n",
            "|funcPK|equipePK|dataPK|cargoPK|salario|quantidadeLancamentos|\n",
            "+------+--------+------+-------+-------+---------------------+\n",
            "|   147|       2|     5|     64|1559.94|                    1|\n",
            "|   124|       2|     5|    329|8102.77|                    1|\n",
            "|   175|       1|     5|    328|2532.51|                    1|\n",
            "|   171|       1|     5|    245| 7882.7|                    1|\n",
            "+------+--------+------+-------+-------+---------------------+\n",
            "only showing top 4 rows\n",
            "\n",
            "+--------+---------+------+--------+---------------------+\n",
            "|equipePK|clientePK|dataPK| receita|quantidadeNegociacoes|\n",
            "+--------+---------+------+--------+---------------------+\n",
            "|       2|        9|    22|11564.75|                    1|\n",
            "|       2|       24|    11| 17990.5|                    1|\n",
            "|       2|       28|    21| 16335.9|                    1|\n",
            "|       1|       30|    23| 8495.55|                    1|\n",
            "+--------+---------+------+--------+---------------------+\n",
            "only showing top 4 rows\n",
            "\n",
            "+--------+-------------+--------------------+--------------+----------------+-----------------+----------------+-----------------+--------------+---------------+\n",
            "|equipePK|   equipeNome|          filialNome|  filialCidade|filialEstadoNome|filialEstadoSigla|filialRegiaoNome|filialRegiaoSigla|filialPaisNome|filialPaisSigla|\n",
            "+--------+-------------+--------------------+--------------+----------------+-----------------+----------------+-----------------+--------------+---------------+\n",
            "|       1|APP - DESKTOP|SAO PAULO - AV. P...|     SAO PAULO|       SAO PAULO|               SP|         SUDESTE|               SE|        BRASIL|             BR|\n",
            "|       2|APP - DESKTOP|RIO DE JANEIRO - ...|RIO DE JANEIRO|  RIO DE JANEIRO|               RJ|         SUDESTE|               SE|        BRASIL|             BR|\n",
            "|       3|          WEB|SAO PAULO - AV. P...|     SAO PAULO|       SAO PAULO|               SP|         SUDESTE|               SE|        BRASIL|             BR|\n",
            "|       4|          WEB|RIO DE JANEIRO - ...|RIO DE JANEIRO|  RIO DE JANEIRO|               RJ|         SUDESTE|               SE|        BRASIL|             BR|\n",
            "+--------+-------------+--------------------+--------------+----------------+-----------------+----------------+-----------------+--------------+---------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resposta da Questão 10\n",
        "# Não se esqueça de comentar detalhadamente a sua solução\n",
        "\n",
        "query= '''SELECT\n",
        "        equ.equipeNome,\n",
        "        avg(salario) as MEDIASALARIO,\n",
        "        avg(receita) as MEDIARECEITA,\n",
        "        avg(salario) - avg(receita) as MEDIA\n",
        "\n",
        "        from equipe equ\n",
        "        inner join negociacao neg on equ.equipePK = neg.equipePK\n",
        "        inner join pagamento pag on equ.equipePK = pag.equipePK\n",
        "        group by equ.equipeNome\n",
        "        order by equ.equipeNome\n",
        "            '''\n",
        "\n",
        "result_df = spark.sql(query)\n",
        "\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFDGVMSL9qB8",
        "outputId": "2fe5dc95-214a-4e55-c8f9-6e7671064736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------------+------------------+\n",
            "|    equipeNome|     MEDIASALARIO|      MEDIARECEITA|\n",
            "+--------------+-----------------+------------------+\n",
            "| APP - DESKTOP|7395.544118067273|17390.081895094652|\n",
            "|  APP - MOBILE|8601.820626016066|14301.106680199791|\n",
            "|BI & ANALYTICS|7578.408653202427| 64232.81567855812|\n",
            "|           WEB|7380.769945513439| 8600.226078192974|\n",
            "+--------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resposta da Questão 10\n",
        "# Não se esqueça de comentar detalhadamente a sua solução\n",
        "\n",
        "query= '''SELECT\n",
        "        equ.equipeNome,\n",
        "        avg(salario) as MEDIASALARIO,\n",
        "        avg(receita) as MEDIARECEITA,\n",
        "        avg(salario) - avg(receita) as MEDIA\n",
        "\n",
        "        from equipe equ\n",
        "        inner join (select equipePK, avg(rececita) as avg_receita from negociacao group by equipePK) neg on equ.equipePK = neg.equipePK\n",
        "        inner join (select avg(pagamento) as avg_receita from negociacao) pag on equ.equipePK = pag.equipePK\n",
        "        group by equ.equipeNome\n",
        "        order by equ.equipeNome\n",
        "            '''\n",
        "\n",
        "result_df = spark.sql(query)\n",
        "\n",
        "result_df.show()"
      ],
      "metadata": {
        "id": "xTXUsJan_WYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resposta da Questão 10\n",
        "# Não se esqueça de comentar detalhadamente a sua solução\n",
        "\n",
        "query= '''\n",
        "\n",
        "with avg_salario as\n",
        "        (SELECT\n",
        "        equipePK,\n",
        "        avg(salario) as MEDIASALARIO\n",
        "\n",
        "        from pagamento pag\n",
        "        group by equipePK),\n",
        "\n",
        "avg_receita as\n",
        "        (SELECT\n",
        "        equipePK,\n",
        "        avg(receita) as MEDIARECEITA\n",
        "\n",
        "        from negociacao neg\n",
        "        group by equipePK)\n",
        "\n",
        "        select\n",
        "        equ.equipeNome as `NOME DA EQUIPE`,\n",
        "        equ.filialNome as `NOME DA FILIAL`,\n",
        "        equ.filialCidade as `CIDADE DA FILIAL`,\n",
        "        round(MEDIARECEITA - MEDIASALARIO, 2) as `LUCRO OU PREJUÍZO`\n",
        "        from equipe as equ\n",
        "        inner join avg_salario as sal on equ.equipePK = sal.equipePK\n",
        "        inner join avg_receita as rec on equ.equipePK = rec.equipePK\n",
        "        where equ.filialRegiaoNome = \"SUDESTE\"\n",
        "        order by `NOME DA EQUIPE`, `NOME DA FILIAL`, `CIDADE DA FILIAL`, `LUCRO OU PREJUÍZO`\n",
        "\n",
        "            '''\n",
        "\n",
        "result_df = spark.sql(query)\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "# NOME DA EQUIPE, NOME DA FILIAL, CIDADE DA FILIAL, LUCRO OU PREJUÍZO."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAZidU9YAmwL",
        "outputId": "6ff8b672-2ee4-4fb7-ed63-56448ba21fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+----------------+-----------------+\n",
            "|NOME DA EQUIPE|      NOME DA FILIAL|CIDADE DA FILIAL|LUCRO OU PREJUÍZO|\n",
            "+--------------+--------------------+----------------+-----------------+\n",
            "|           WEB|RIO DE JANEIRO - ...|  RIO DE JANEIRO|          -370.04|\n",
            "|  APP - MOBILE|RIO DE JANEIRO - ...|  RIO DE JANEIRO|          5828.01|\n",
            "| APP - DESKTOP|RIO DE JANEIRO - ...|  RIO DE JANEIRO|         10452.61|\n",
            "|  APP - MOBILE|SAO PAULO - AV. P...|       SAO PAULO|          5642.31|\n",
            "|           WEB|SAO PAULO - AV. P...|       SAO PAULO|           680.37|\n",
            "| APP - DESKTOP|SAO PAULO - AV. P...|       SAO PAULO|           9511.4|\n",
            "|BI & ANALYTICS|SAO PAULO - AV. P...|       SAO PAULO|         55019.53|\n",
            "+--------------+--------------------+----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}